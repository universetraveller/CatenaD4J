# Analyze Tests
## Overview
Scripts under this folder are used to extract assertion related identifiers (e.g. assertTrue) from the trigger tests.  

Notice the manual processes in this experiment is to further promote the precision and accuracy of the result. The automatically generated result can also be used in the later experiments.  

In this phase, original Defects4J multi-hunk bugs are divided into two parts (The corresponding bug files are `../construct_database/2toMore6` and `../construct_database/2toMore11`.). The first part contains bugs in six projects, `Chart`, `Lang`, `Math`, `Time`, `Closure` and `Mockito`. The second part contains bugs in the other 11 projects, `Cli`, `Codec`, `Collections`, `Compress`, `Csv`, `Gson`, `JacksonCore`, `JacksonDatabind`, `JacksonXml`, `Jsoup` and `JxPath`. The reason for division is as below:  

After manually checking the identifiers extracted from all Defects4J projects, we found an identifier, `parse` appear in two projects Closure and Cli with distinct meanings: in Closure the identifier is related to assertion, while in Cli it has no relation to assertion. To ensure more accurate identification of assertion-related identifiers, and considering that bugs from the divided six projects had already been analyzed in the previous version of CatenaD4J, we divided the original multi-hunk Defects4J bugs into these two parts.  

## Usage
### Extract identifiers from Junit lib
Script `extract_junit_asserts.py` can extract assertion related identifiers from official Junit 3.8 and Junit 4 source code (specifically, from the class Assert).
`python3 extract_junit_asserts.py`  

The output is `./names/junit` (a text file in which every line is a assertion related identifier).  

### Analyze the trigger tests
Script [analyze\_ver\_5.py](./analyze_ver_5.py) is the analysis process. Name ver\_5 indicates there are other versions before it is developed. Actually, the result we used in the later experiments is combined from results of whole versions. We select version 5 there because it is good in both precision and accuracy.  
`python3 analyze_ver_5.py <path_to_metadata_json_file> <path_to_bug_ids> <path_to_bugs>`  

For actual arguments please check the previous experiments and ensure they are done.  

The paths of files/directories in arguments can be specified in the experiment `construct_database` (by default, they are `../construct_database/d4j_export/database.json`, `../construct_database/2toMore` and `/tmp` respectively). To analyze the bugs which are divied into two parts as mentioned in the overview, the script should be run twice and the argument `<path_to_bug_ids>` should be `../construct_database/2toMore6` and `../construct_database/2toMore11` respectively.

The output is `./logs/log5` which contains the runtime log and the identifiers filtered by the script. You should change the file name of `./logs/log5` if you run the script multiple times and each time on different parts of the bugs (see `./run.sh` for reference where `./logs/log5_6` refers to the result of bugs in six projects and `./logs/log5_6` refers to the result of bugs in the other 11 projects). However, the result (a identifier list used in the later experiments) should be manually checked and written (the script only filters all suspicious identifiers and it may make mistakes).  

### Analyze stack trace of the trigger tests
Script `analyze_triggers.py` further extracts identifiers that may cause test failed (even they are not an assertion).  
`python3 analyze_triggers.py <path_to_bug_ids> [<path_to_defects4j>]`  

The output is a log file `./names/tryTrigger` which is similar to file `./logs/log5` that we should check its result manually. You should change the file name of `./names/tryTrigger` if you run the script multiple times and each time on different parts of the bugs (see `./run.sh` for reference where `./names/tryTrigger6` refers to the result of bugs in six projects and `./names/tryTrigger11` refers to the result of bugs in the other 11 projects).  

### Generate identifiers list
Scripts and text files under folder `./names` are for manually checking and identifiers list generation. 

The text files contain the manual analysis from the previous logs generated by scripts. The process is to check every identifier in the logs if it is a method that may cause test failed, if so, add it to one of the text files.  

The scripts are written for partially automate the manual analysis process.  

To generate the identifiers list, run script `./names/table.py` which generate two text files `./names/table6` and `./names/table11` that can be used in the later experiments. However, it is recommanded to double check the file content.  

## Reproduce the experiments
The whole process takes about 45 seconds.  

* Ensure you have finished the previous experiment [construct\_database](../construct_database)  

* Run script `./run.sh` or refer to the usage section to execute scripts   
